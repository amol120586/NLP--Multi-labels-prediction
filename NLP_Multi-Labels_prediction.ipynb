{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import re\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(r'F:\\DataSets\\Akaike Technologies\\train_data.csv')\n",
    "train_label = pd.read_csv(r'F:\\DataSets\\Akaike Technologies\\train_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(r'F:\\DataSets\\Akaike Technologies\\test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of missing values\n",
    "\n",
    "train_data.isnull().sum()\n",
    "train_label.isnull().sum()\n",
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeldata Categorization - Making dummy features & groupby by 'id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = pd.get_dummies(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train_label.groupby(['id'], as_index=False).agg('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categories = list((train_label.drop(['id'], axis=1)).columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge two dataframes: train_data & train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.merge(train_data, train_label, on='id')       #df_train = Final train dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning of text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"can not \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = re.sub(r'[?|!|\\'|\"|#]',r'',text)\n",
    "    text = re.sub(r'[.|,|)|(|\\|/]',r'',text)\n",
    "    text = text.strip(' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train['text'] = df_train['text'].map(lambda com : clean_text(com))\n",
    "df_test['text'] = df_test['text'].map(lambda com: clean_text(com))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into train & test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_train split into train and test data , making the variables 'X_train' & ' X_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(df_train, random_state=42, test_size=0.20, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.text\n",
    "X_test = test.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                ('clf', OneVsRestClassifier(MultinomialNB(\n",
    "                    fit_prior=True, class_prior=None))),\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in categories:\n",
    "    print('... Processing {}'.format(category))\n",
    "    # train the model using X_dtm & y\n",
    "    NB_pipeline.fit(X_train, train[category])\n",
    "    # compute the testing accuracy\n",
    "    prediction = NB_pipeline.predict(X_test)\n",
    "    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = []                                                #list to collect test_prediction list.\n",
    "LogReg_pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=1)),\n",
    "            ])\n",
    "for category in categories:\n",
    "    test_prediction = []\n",
    "    print('... Processing {}'.format(category))\n",
    "    # train the model using X_dtm & y\n",
    "    LogReg_pipeline.fit(X_train, train[category])\n",
    "    # compute the testing accuracy\n",
    "    prediction = LogReg_pipeline.predict(X_test)\n",
    "    \n",
    "    test_prediction = LogReg_pipeline.predict(df_test.text)       #Test_data prediction \n",
    "    test_result.append(test_prediction)\n",
    "    \n",
    "    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame.from_dict(dict( zip( categories, test_result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = df_result.set_index(df_test.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.to_csv('C:\\Users\\Mangesh\\Desktop\\submission.csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = df_train.text.str.len()\n",
    "lens.hist(bins = np.arange(0,6000,5))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can see few \"df.train['text']\" data length is beyond 5000 characters. They can be considered as 'outliers'\n",
    "\n",
    "df_train[df_train.text.str.len()>5000]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
